{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install Package"
      ],
      "metadata": {
        "id": "zzukVRGk3hhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7d5EHLNE7zX",
        "outputId": "98c0b48c-fda2-4b55-fa01-a8b97909bea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m174.1/232.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Library"
      ],
      "metadata": {
        "id": "0wB2F6gJ3tky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import PyPDF2\n",
        "from google.colab import files\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import pipeline\n",
        "from sklearn.metrics import jaccard_score"
      ],
      "metadata": {
        "id": "hhqpy77q2zGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Upload Files"
      ],
      "metadata": {
        "id": "7j2QZaxo4USk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()\n",
        "\n",
        "valid_skills = {\n",
        "    \"python\", \"java\", \"c++\", \"c#\", \"r\", \"kotlin\", \"javascript\", \"php\", \"html\", \"css\", \"ruby\",\n",
        "    \"tensorflow\", \"keras\", \"scikit-learn\", \"pandas\", \"numpy\", \"matplotlib\", \"seaborn\", \"django\", \"flask\", \"spring\",\n",
        "    \"sql\", \"mysql\", \"postgresql\", \"microsoft sql\", \"nosql\", \"mongodb\", \"apache spark\", \"hadoop\", \"power bi\", \"tableau\", \"excel\",\n",
        "    \"google cloud platform\", \"aws\", \"machine learning\", \"deep learning\", \"neural networks\", \"natural language processing\",\n",
        "    \"nlp\", \"computer vision\", \"random forest\", \"decision trees\", \"regression\", \"classification\", \"clustering\",\n",
        "    \"big data\", \"google bigquery\", \"kubernetes\", \"docker\", \"heroku\", \"azure\", \"time series analysis\", \"anova\", \"statistics\",\n",
        "    \"data mining\", \"data analysis\", \"raspberry pi\", \"automation\", \"git\", \"github\", \"jira\", \"restapi\", \"selenium\",\n",
        "    \"ci/cd pipelines\", \"jenkins\", \"tableau\",\"apache kafka\", \"apache flink\", \"airflow\", \"luigi\", \"etl\",\n",
        "    \"google kubernetes engine\", \"amazon elastic kubernetes service\", \"microsoft azure functions\",\n",
        "    \"terraform\", \"ansible\", \"chef\", \"puppet\", \"vagrant\",\n",
        "    \"plotly\", \"d3.js\", \"power bi\", \"qlikview\",\n",
        "    \"bitbucket\", \"gitlab\",\n",
        "    \"amazon s3\", \"google cloud storage\", \"azure blob storage\",\n",
        "    \"apache hadoop\", \"apache cassandra\", \"apache hive\", \"apache pig\",\n",
        "    \"swift\", \"rust\", \"go\",\n",
        "    \"node.js\", \"express.js\", \"vue.js\", \"angular\",\n",
        "    \"reinforcement learning\", \"generative adversarial networks\", \"gan\", \"bayesian networks\",\n",
        "    \"mqtt\", \"zigbee\", \"lorawan\", \"ble\", \"bluetooth low energy\",\n",
        "    \"ethereum\", \"hyperledger\", \"smart contracts\", \"solidity\",\n",
        "    \"ethical hacking\", \"penetration testing\", \"siem\", \"firewalls\", \"vpn\",\n",
        "    \"flutter\", \"react native\", \"swift\", \"android sdk\",\n",
        "    \"sap businessobjects\", \"microstrategy\", \"sas business analytics\",\n",
        "    \"unity\", \"unreal engine\", \"cocos2d\",\n",
        "    \"opencv\", \"yolo\", \"mask r-cnn\", \"image segmentation\"\n",
        "}"
      ],
      "metadata": {
        "id": "fS3DQa8c4a7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "437d085b-535c-4ada-84d3-d167dc490b67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-98a514f2-e8fe-4961-9d92-8b4c35b4fc90\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-98a514f2-e8fe-4961-9d92-8b4c35b4fc90\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving candidate_000.pdf to candidate_000.pdf\n",
            "Saving candidate_001.pdf to candidate_001.pdf\n",
            "Saving candidate_002.pdf to candidate_002.pdf\n",
            "Saving candidate_003.pdf to candidate_003.pdf\n",
            "Saving candidate_004.pdf to candidate_004.pdf\n",
            "Saving candidate_005.pdf to candidate_005.pdf\n",
            "Saving candidate_006.pdf to candidate_006.pdf\n",
            "Saving candidate_007.pdf to candidate_007.pdf\n",
            "Saving candidate_008.pdf to candidate_008.pdf\n",
            "Saving candidate_009.pdf to candidate_009.pdf\n",
            "Saving candidate_010.pdf to candidate_010.pdf\n",
            "Saving candidate_011.pdf to candidate_011.pdf\n",
            "Saving candidate_012.pdf to candidate_012.pdf\n",
            "Saving candidate_013.pdf to candidate_013.pdf\n",
            "Saving candidate_014.pdf to candidate_014.pdf\n",
            "Saving candidate_015.pdf to candidate_015.pdf\n",
            "Saving candidate_016.pdf to candidate_016.pdf\n",
            "Saving candidate_017.pdf to candidate_017.pdf\n",
            "Saving candidate_018.pdf to candidate_018.pdf\n",
            "Saving candidate_019.pdf to candidate_019.pdf\n",
            "Saving candidate_020.pdf to candidate_020.pdf\n",
            "Saving candidate_021.pdf to candidate_021.pdf\n",
            "Saving candidate_022.pdf to candidate_022.pdf\n",
            "Saving candidate_023.pdf to candidate_023.pdf\n",
            "Saving candidate_024.pdf to candidate_024.pdf\n",
            "Saving candidate_025.pdf to candidate_025.pdf\n",
            "Saving candidate_026.pdf to candidate_026.pdf\n",
            "Saving candidate_027.pdf to candidate_027.pdf\n",
            "Saving candidate_028.pdf to candidate_028.pdf\n",
            "Saving candidate_029.pdf to candidate_029.pdf\n",
            "Saving candidate_030.pdf to candidate_030.pdf\n",
            "Saving candidate_031.pdf to candidate_031.pdf\n",
            "Saving candidate_032.pdf to candidate_032.pdf\n",
            "Saving candidate_033.pdf to candidate_033.pdf\n",
            "Saving candidate_034.pdf to candidate_034.pdf\n",
            "Saving candidate_035.pdf to candidate_035.pdf\n",
            "Saving candidate_036.pdf to candidate_036.pdf\n",
            "Saving candidate_037.pdf to candidate_037.pdf\n",
            "Saving candidate_038.pdf to candidate_038.pdf\n",
            "Saving candidate_039.pdf to candidate_039.pdf\n",
            "Saving candidate_040.pdf to candidate_040.pdf\n",
            "Saving candidate_041.pdf to candidate_041.pdf\n",
            "Saving candidate_042.pdf to candidate_042.pdf\n",
            "Saving candidate_043.pdf to candidate_043.pdf\n",
            "Saving candidate_044.pdf to candidate_044.pdf\n",
            "Saving candidate_045.pdf to candidate_045.pdf\n",
            "Saving candidate_046.pdf to candidate_046.pdf\n",
            "Saving candidate_047.pdf to candidate_047.pdf\n",
            "Saving candidate_048.pdf to candidate_048.pdf\n",
            "Saving candidate_049.pdf to candidate_049.pdf\n",
            "Saving candidate_050.pdf to candidate_050.pdf\n",
            "Saving candidate_051.pdf to candidate_051.pdf\n",
            "Saving candidate_052.pdf to candidate_052.pdf\n",
            "Saving candidate_053.pdf to candidate_053.pdf\n",
            "Saving candidate_054.pdf to candidate_054.pdf\n",
            "Saving candidate_055.pdf to candidate_055.pdf\n",
            "Saving candidate_056.pdf to candidate_056.pdf\n",
            "Saving candidate_057.pdf to candidate_057.pdf\n",
            "Saving candidate_058.pdf to candidate_058.pdf\n",
            "Saving candidate_059.pdf to candidate_059.pdf\n",
            "Saving candidate_060.pdf to candidate_060.pdf\n",
            "Saving candidate_061.pdf to candidate_061.pdf\n",
            "Saving candidate_062.pdf to candidate_062.pdf\n",
            "Saving candidate_063.pdf to candidate_063.pdf\n",
            "Saving candidate_064.pdf to candidate_064.pdf\n",
            "Saving candidate_065.pdf to candidate_065.pdf\n",
            "Saving candidate_066.pdf to candidate_066.pdf\n",
            "Saving candidate_067.pdf to candidate_067.pdf\n",
            "Saving candidate_068.pdf to candidate_068.pdf\n",
            "Saving candidate_069.pdf to candidate_069.pdf\n",
            "Saving candidate_070.pdf to candidate_070.pdf\n",
            "Saving candidate_071.pdf to candidate_071.pdf\n",
            "Saving candidate_072.pdf to candidate_072.pdf\n",
            "Saving candidate_073.pdf to candidate_073.pdf\n",
            "Saving candidate_074.pdf to candidate_074.pdf\n",
            "Saving candidate_075.pdf to candidate_075.pdf\n",
            "Saving candidate_076.pdf to candidate_076.pdf\n",
            "Saving candidate_077.pdf to candidate_077.pdf\n",
            "Saving candidate_078.pdf to candidate_078.pdf\n",
            "Saving candidate_079.pdf to candidate_079.pdf\n",
            "Saving candidate_080.pdf to candidate_080.pdf\n",
            "Saving candidate_081.pdf to candidate_081.pdf\n",
            "Saving candidate_082.pdf to candidate_082.pdf\n",
            "Saving candidate_083.pdf to candidate_083.pdf\n",
            "Saving candidate_084.pdf to candidate_084.pdf\n",
            "Saving candidate_085.pdf to candidate_085.pdf\n",
            "Saving candidate_086.pdf to candidate_086.pdf\n",
            "Saving candidate_087.pdf to candidate_087.pdf\n",
            "Saving candidate_088.pdf to candidate_088.pdf\n",
            "Saving candidate_089.pdf to candidate_089.pdf\n",
            "Saving candidate_090.pdf to candidate_090.pdf\n",
            "Saving candidate_091.pdf to candidate_091.pdf\n",
            "Saving candidate_092.pdf to candidate_092.pdf\n",
            "Saving candidate_093.pdf to candidate_093.pdf\n",
            "Saving candidate_094.pdf to candidate_094.pdf\n",
            "Saving candidate_095.pdf to candidate_095.pdf\n",
            "Saving candidate_096.pdf to candidate_096.pdf\n",
            "Saving candidate_097.pdf to candidate_097.pdf\n",
            "Saving candidate_098.pdf to candidate_098.pdf\n",
            "Saving candidate_099.pdf to candidate_099.pdf\n",
            "Saving candidate_100.pdf to candidate_100.pdf\n",
            "Saving candidate_101.pdf to candidate_101.pdf\n",
            "Saving candidate_102.pdf to candidate_102.pdf\n",
            "Saving candidate_103.pdf to candidate_103.pdf\n",
            "Saving candidate_104.pdf to candidate_104.pdf\n",
            "Saving candidate_105.pdf to candidate_105.pdf\n",
            "Saving candidate_106.pdf to candidate_106.pdf\n",
            "Saving candidate_107.pdf to candidate_107.pdf\n",
            "Saving candidate_108.pdf to candidate_108.pdf\n",
            "Saving candidate_109.pdf to candidate_109.pdf\n",
            "Saving candidate_110.pdf to candidate_110.pdf\n",
            "Saving candidate_111.pdf to candidate_111.pdf\n",
            "Saving candidate_112.pdf to candidate_112.pdf\n",
            "Saving candidate_113.pdf to candidate_113.pdf\n",
            "Saving candidate_114.pdf to candidate_114.pdf\n",
            "Saving candidate_115.pdf to candidate_115.pdf\n",
            "Saving candidate_116.pdf to candidate_116.pdf\n",
            "Saving candidate_117.pdf to candidate_117.pdf\n",
            "Saving candidate_118.pdf to candidate_118.pdf\n",
            "Saving candidate_119.pdf to candidate_119.pdf\n",
            "Saving candidate_120.pdf to candidate_120.pdf\n",
            "Saving candidate_121.pdf to candidate_121.pdf\n",
            "Saving candidate_122.pdf to candidate_122.pdf\n",
            "Saving candidate_123.pdf to candidate_123.pdf\n",
            "Saving candidate_124.pdf to candidate_124.pdf\n",
            "Saving candidate_125.pdf to candidate_125.pdf\n",
            "Saving candidate_126.pdf to candidate_126.pdf\n",
            "Saving candidate_127.pdf to candidate_127.pdf\n",
            "Saving candidate_128.pdf to candidate_128.pdf\n",
            "Saving candidate_129.pdf to candidate_129.pdf\n",
            "Saving candidate_130.pdf to candidate_130.pdf\n",
            "Saving candidate_131.pdf to candidate_131.pdf\n",
            "Saving candidate_132.pdf to candidate_132.pdf\n",
            "Saving candidate_133.pdf to candidate_133.pdf\n",
            "Saving candidate_134.pdf to candidate_134.pdf\n",
            "Saving candidate_135.pdf to candidate_135.pdf\n",
            "Saving candidate_136.pdf to candidate_136.pdf\n",
            "Saving candidate_137.pdf to candidate_137.pdf\n",
            "Saving candidate_138.pdf to candidate_138.pdf\n",
            "Saving candidate_139.pdf to candidate_139.pdf\n",
            "Saving candidate_140.pdf to candidate_140.pdf\n",
            "Saving candidate_141.pdf to candidate_141.pdf\n",
            "Saving candidate_142.pdf to candidate_142.pdf\n",
            "Saving candidate_143.pdf to candidate_143.pdf\n",
            "Saving candidate_144.pdf to candidate_144.pdf\n",
            "Saving candidate_145.pdf to candidate_145.pdf\n",
            "Saving candidate_146.pdf to candidate_146.pdf\n",
            "Saving candidate_147.pdf to candidate_147.pdf\n",
            "Saving candidate_148.pdf to candidate_148.pdf\n",
            "Saving candidate_149.pdf to candidate_149.pdf\n",
            "Saving job_list.csv to job_list.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Extract Text from PDF"
      ],
      "metadata": {
        "id": "eotlOmio4dvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    try:\n",
        "        with open(pdf_path, \"rb\") as f:\n",
        "            pdf_reader = PyPDF2.PdfReader(f)\n",
        "            text = \"\"\n",
        "            for page in pdf_reader.pages:\n",
        "                text += page.extract_text()\n",
        "        return text\n",
        "    except:\n",
        "        return \"\"  # Return empty string if any error occurs"
      ],
      "metadata": {
        "id": "41ZVWiZR42MS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Extract Skills from PDF"
      ],
      "metadata": {
        "id": "6VSkKh_c43Da"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_skills_from_text(text, keyword_list):\n",
        "    words = text.lower().split()  # Tokenize text by spaces and convert to lowercase\n",
        "    found_skills = [word for word in words if word in keyword_list]\n",
        "    return list(set(found_skills))  # Return unique skills"
      ],
      "metadata": {
        "id": "UV3sNUPr5Hxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Process Uploaded CVs and Add New Skills to valid_skills"
      ],
      "metadata": {
        "id": "WU89Rlie5Qe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv_data = []\n",
        "updated = True  # Flag to check if any update in skills happened\n",
        "\n",
        "while updated:  # Continue looping if any new skills were added\n",
        "    updated = False  # Reset the flag for this loop\n",
        "    for filename in uploaded:\n",
        "        if filename.endswith(\".pdf\"):\n",
        "            cv_text = extract_text_from_pdf(filename)  # Extract text from the PDF file\n",
        "            if cv_text:\n",
        "                skills = extract_skills_from_text(cv_text, valid_skills)  # Extract skills from the text\n",
        "                cv_data.append({\"filename\": filename, \"skills\": skills})\n",
        "\n",
        "                # Add any new skills found in the CV to valid_skills\n",
        "                new_skills = [skill for skill in skills if skill not in valid_skills]\n",
        "                if new_skills:  # If there are new skills\n",
        "                    valid_skills.update(new_skills)  # Add new skills to valid_skills\n",
        "                    updated = True  # Set the flag to True if new skills were added\n",
        "                    break  # Exit the loop to re-process all CVs again\n",
        "\n",
        "# Convert CV data to DataFrame\n",
        "df_cv = pd.DataFrame(cv_data)\n",
        "\n",
        "# Display DataFrame with extracted skills\n",
        "print(df_cv)\n",
        "\n",
        "# Display updated valid_skills\n",
        "print(\"\\nUpdated valid skills:\", valid_skills)"
      ],
      "metadata": {
        "id": "lMVK2IyT5bU6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec76be15-5b61-4d5a-ea7c-1684675be170"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              filename                                    skills\n",
            "0    candidate_000.pdf        [classification, pandas, excel, r]\n",
            "1    candidate_001.pdf                             [nlp, hadoop]\n",
            "2    candidate_002.pdf                           [flask, python]\n",
            "3    candidate_003.pdf                                    [mqtt]\n",
            "4    candidate_004.pdf                                        []\n",
            "..                 ...                                       ...\n",
            "145  candidate_145.pdf                                        []\n",
            "146  candidate_146.pdf                       [classification, r]\n",
            "147  candidate_147.pdf                       [classification, r]\n",
            "148  candidate_148.pdf  [classification, clustering, regression]\n",
            "149  candidate_149.pdf                              [tensorflow]\n",
            "\n",
            "[150 rows x 2 columns]\n",
            "\n",
            "Updated valid skills: {'anova', 'sql', 'decision trees', 'statistics', 'docker', 'smart contracts', 'postgresql', 'data mining', 'php', 'vagrant', 'mqtt', 'natural language processing', 'apache spark', 'plotly', 'html', 'express.js', 'ble', 'mysql', 'siem', 'heroku', 'tableau', 'machine learning', 'apache cassandra', 'kubernetes', 'selenium', 'sas business analytics', 'google kubernetes engine', 'ethical hacking', 'chef', 'git', 'random forest', 'data analysis', 'apache hadoop', 'kotlin', 'numpy', 'qlikview', 'microstrategy', 'amazon s3', 'solidity', 'terraform', 'ethereum', 'gitlab', 'vpn', 'node.js', 'reinforcement learning', 'python', 'microsoft sql', 'time series analysis', 'generative adversarial networks', 'django', 'cocos2d', 'neural networks', 'excel', 'ruby', 'tensorflow', 'github', 'classification', 'big data', 'c++', 'nlp', 'azure', 'luigi', 'raspberry pi', 'apache hive', 'deep learning', 'google cloud storage', 'angular', 'unreal engine', 'mongodb', 'airflow', 'ansible', 'bitbucket', 'css', 'seaborn', 'r', 'keras', 'lorawan', 'c#', 'bluetooth low energy', 'matplotlib', 'd3.js', 'regression', 'amazon elastic kubernetes service', 'go', 'apache kafka', 'opencv', 'flutter', 'javascript', 'image segmentation', 'firewalls', 'sap businessobjects', 'bayesian networks', 'jenkins', 'swift', 'microsoft azure functions', 'java', 'rust', 'penetration testing', 'mask r-cnn', 'unity', 'apache flink', 'hyperledger', 'yolo', 'restapi', 'android sdk', 'power bi', 'scikit-learn', 'nosql', 'azure blob storage', 'ci/cd pipelines', 'hadoop', 'etl', 'vue.js', 'jira', 'computer vision', 'google bigquery', 'apache pig', 'react native', 'google cloud platform', 'flask', 'puppet', 'pandas', 'clustering', 'automation', 'gan', 'aws', 'spring', 'zigbee'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the DataFrame to a JSON file\n",
        "df_cv.to_json(\"cv_data.json\", orient=\"records\", lines=True)\n",
        "\n",
        "print(\"DataFrame has been saved as cv_data.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RGi1cyxEtc7",
        "outputId": "6cfc5962-eecf-4762-8d37-ff631c4280fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame has been saved as cv_data.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Process Job List"
      ],
      "metadata": {
        "id": "4DPiIiB85jK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load job list\n",
        "df_jobs = pd.read_csv('job_list.csv')\n",
        "\n",
        "# Keywords for job titles and corresponding skills\n",
        "job_keywords = {\n",
        "    \"data\": [\"python\", \"sql\", \"data analysis\", \"statistics\", \"excel\", \"power bi\", \"tableau\"],\n",
        "    \"analyst\": [\"sql\", \"python\", \"data visualization\", \"business analysis\", \"statistics\"],\n",
        "    \"engineering\": [\"python\", \"sql\", \"java\", \"c++\", \"cloud computing\", \"docker\", \"kubernetes\"],\n",
        "    \"manager\": [\"project management\", \"excel\", \"jira\", \"leadership\", \"communication\"],\n",
        "    \"designer\": [\"photoshop\", \"illustrator\", \"ui/ux\", \"adobe xd\", \"figma\"],\n",
        "    \"electrical\": [\"circuit design\", \"matlab\", \"simulink\", \"power systems\"],\n",
        "    \"illustrator\": [\"illustrator\", \"photoshop\", \"adobe suite\"],\n",
        "    \"application\": [\"application development\", \"java\", \"android\", \"ios\", \"flutter\"],\n",
        "    \"developer\": [\"java\", \"javascript\", \"python\", \"html\", \"css\", \"react\", \"node.js\"],\n",
        "    \"business\": [\"business analysis\", \"strategy\", \"excel\", \"data analysis\", \"communication\"]\n",
        "}\n",
        "\n",
        "# Function to generate skills_needed based on job_title\n",
        "def generate_skills_needed(job_title):\n",
        "    skills_needed = []\n",
        "    for keyword, skills in job_keywords.items():\n",
        "        if keyword in job_title.lower():\n",
        "            skills_needed.extend(skills)\n",
        "    return list(set(skills_needed))\n",
        "\n",
        "df_jobs[\"skills_needed\"] = df_jobs[\"job_title\"].apply(generate_skills_needed)"
      ],
      "metadata": {
        "id": "9Pzuojw25sKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate skills_needed based on job_title\n",
        "def generate_skills_needed(job_title):\n",
        "    skills_needed = []\n",
        "    for keyword, skills in job_keywords.items():\n",
        "        if keyword in job_title.lower():\n",
        "            skills_needed.extend(skills)\n",
        "    return list(set(skills_needed))\n",
        "\n",
        "df_jobs[\"skills_needed\"] = df_jobs[\"job_title\"].apply(generate_skills_needed)"
      ],
      "metadata": {
        "id": "1KUKQaaK30Xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Pre-Process for TensorFlow Model"
      ],
      "metadata": {
        "id": "nLNqy8HX5xzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize MultiLabelBinarizer for encoding skills\n",
        "mlb = MultiLabelBinarizer()\n",
        "\n",
        "# Transform the skills in CVs and job requirements into binary format\n",
        "cv_skills_matrix = mlb.fit_transform(df_cv[\"skills\"].values)\n",
        "job_skills_matrix = mlb.fit_transform(df_jobs[\"skills_needed\"].values)\n",
        "\n",
        "# Generate training data (cartesian product of CVs and jobs)\n",
        "X = []  # Features\n",
        "y = []  # Labels\n",
        "\n",
        "for cv_idx, cv_skills in enumerate(cv_skills_matrix):\n",
        "    for job_idx, job_skills in enumerate(job_skills_matrix):\n",
        "        # Combine CV and job skills into a single feature vector\n",
        "        X.append(np.concatenate([cv_skills, job_skills]))  # Combine CV and job skill features\n",
        "\n",
        "        # Label as 1 if there's a significant overlap between CV and job skills, otherwise 0\n",
        "        overlap_count = len(set(cv_skills).intersection(set(job_skills)))  # Count the overlap\n",
        "        y.append(1 if overlap_count > 0 else 0)  # If overlap, label 1, else 0\n",
        "\n",
        "# Convert X and y to numpy arrays\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "4NVYKUZm5_cY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Build and Train the Model"
      ],
      "metadata": {
        "id": "Oc86gMlW6AsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(2, activation=\"relu\", input_shape=(X.shape[1],)),  # Hidden layer with 2 neurons\n",
        "    tf.keras.layers.Dropout(0.2),  # Dropout for regularization to reduce overfitting\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")  # Output layer: binary classification (1 or 0)\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# EarlyStopping callback to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model with the training data, using validation data\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=32, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Model accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0L_9EPtDsxs",
        "outputId": "0d77637d-c91d-4ae5-b3e4-8d0e225432fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9410 - loss: 0.4204 - val_accuracy: 1.0000 - val_loss: 0.0481\n",
            "Epoch 2/3\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.1167 - val_accuracy: 1.0000 - val_loss: 0.0165\n",
            "Epoch 3/3\n",
            "\u001b[1m672/672\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0770 - val_accuracy: 1.0000 - val_loss: 0.0074\n",
            "\u001b[1m168/168\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 0.0073\n",
            "Model accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8: Generate Job Recommendations Using Cosine Similarity"
      ],
      "metadata": {
        "id": "IpWKm4Zs6Lfy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "recommendations = []\n",
        "\n",
        "for _, cv_row in df_cv.iterrows():\n",
        "    # Mengubah keterampilan kandidat menjadi format yang sesuai dengan MultiLabelBinarizer\n",
        "    candidate_skills = mlb.transform([cv_row[\"skills\"]])[0]\n",
        "    scores = []\n",
        "\n",
        "    for job_idx, job_skills in enumerate(job_skills_matrix):\n",
        "        # Menghitung cosine similarity antara keterampilan kandidat dan keterampilan pekerjaan\n",
        "        similarity = cosine_similarity([candidate_skills], [job_skills])[0][0]\n",
        "        scores.append((job_idx, similarity))\n",
        "\n",
        "    # Mendapatkan 3 pekerjaan teratas berdasarkan kesamaan (similarity)\n",
        "    top_3_jobs = sorted(scores, key=lambda x: x[1], reverse=True)[:3]\n",
        "    recommendations.append({\n",
        "        \"filename\": cv_row[\"filename\"],\n",
        "        \"recommendations\": [\n",
        "            {\n",
        "                \"job_title\": df_jobs.loc[job[0], \"job_title\"],\n",
        "                \"job_link\": df_jobs.loc[job[0], \"job_link\"],\n",
        "                \"similarity\": job[1]\n",
        "            }\n",
        "            for job in top_3_jobs\n",
        "        ]\n",
        "    })"
      ],
      "metadata": {
        "id": "S3zpG2fB6Vu0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4303278-32f5-491f-c680-a88d92b82691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['classification', 'pandas', 'r'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['hadoop', 'nlp'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['flask'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['mqtt'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['aws', 'r'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['gan'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['azure'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['classification', 'nlp'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['nlp', 'r'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['azure', 'numpy', 'pandas'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['clustering'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['r'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['nlp'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['automation'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['r'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['r'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['regression'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['automation'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['classification'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['r'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['classification'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['r'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['aws', 'nlp'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['opencv'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['tensorflow'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['automation', 'keras', 'nlp', 'r'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['classification'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['r', 'regression'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['classification'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['classification', 'clustering', 'nlp'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['keras', 'nlp', 'r', 'tensorflow'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['nlp'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['nlp'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['r'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['aws'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['nlp'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['automation', 'aws'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['selenium'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['classification', 'clustering', 'flask', 'nlp'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['r', 'regression'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['azure'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['r'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['keras', 'opencv'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['automation', 'r'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['r'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['r'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['classification'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['aws', 'classification', 'flask', 'r'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['aws'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['etl'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['r'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['r'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['classification', 'r'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['nlp'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['r'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['r', 'regression'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['classification', 'regression'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['hadoop', 'nlp'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['yolo'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['aws', 'regression'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['clustering'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['r'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['regression'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['go'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['classification', 'regression'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['classification', 'r'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['classification'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['aws'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['classification'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['aws', 'azure'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['r'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['regression'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['r'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['automation'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['aws', 'nlp'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['classification', 'clustering', 'r'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['pandas'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['nlp'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['nlp'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['nlp'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['opencv', 'r'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['yolo'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['azure', 'nlp', 'r'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['nlp'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['classification', 'r'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['classification', 'r'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['classification', 'clustering', 'regression'] will be ignored\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['tensorflow'] will be ignored\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 9: Display Results"
      ],
      "metadata": {
        "id": "ZjBriW6L6WzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "expanded_recommendations = []\n",
        "\n",
        "for row in recommendations:\n",
        "    recs = row[\"recommendations\"]\n",
        "    filename = row[\"filename\"]\n",
        "    rec_data = {\"filename\": filename}\n",
        "\n",
        "    # Ambil 3 rekomendasi teratas dan pisahkan ke dalam kolom yang berbeda\n",
        "    for i, rec in enumerate(recs[:3]):  # Hanya mengambil 3 rekomendasi teratas\n",
        "        rec_data[f\"recommendation_{i+1}\"] = rec[\"job_title\"]\n",
        "        rec_data[f\"link_{i+1}\"] = rec[\"job_link\"]\n",
        "        rec_data[f\"similarity_{i+1}\"] = rec[\"similarity\"]\n",
        "\n",
        "    expanded_recommendations.append(rec_data)\n",
        "\n",
        "df_expanded = pd.DataFrame(expanded_recommendations)\n",
        "print(df_expanded.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HO1nfMktDts0",
        "outputId": "15b5501a-1fe1-4929-d75b-277b41beaabf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            filename                                   recommendation_1  \\\n",
            "0  candidate_000.pdf                             Outage Manager - Texas   \n",
            "1  candidate_001.pdf  Sr. Principal Analyst – Data Analytics & AI in...   \n",
            "2  candidate_002.pdf                System Analytics - Pharmacy Analyst   \n",
            "3  candidate_003.pdf  Sr. Principal Analyst – Data Analytics & AI in...   \n",
            "4  candidate_004.pdf  Sr. Principal Analyst – Data Analytics & AI in...   \n",
            "\n",
            "                                              link_1  similarity_1  \\\n",
            "0  https://www.indeed.com/rc/clk?jk=0209edfacd6ac...      0.447214   \n",
            "1  https://www.indeed.com/rc/clk?jk=f2ee679edfca4...      0.000000   \n",
            "2  https://www.indeed.com/rc/clk?jk=874d8ef92e7ed...      0.447214   \n",
            "3  https://www.indeed.com/rc/clk?jk=f2ee679edfca4...      0.000000   \n",
            "4  https://www.indeed.com/rc/clk?jk=f2ee679edfca4...      0.000000   \n",
            "\n",
            "                             recommendation_2  \\\n",
            "0  Technical Service Support Manager - Remote   \n",
            "1         System Analytics - Pharmacy Analyst   \n",
            "2                     Utility Billing Analyst   \n",
            "3         System Analytics - Pharmacy Analyst   \n",
            "4         System Analytics - Pharmacy Analyst   \n",
            "\n",
            "                                              link_2  similarity_2  \\\n",
            "0  https://www.indeed.com/rc/clk?jk=05601e11b37a3...      0.447214   \n",
            "1  https://www.indeed.com/rc/clk?jk=874d8ef92e7ed...      0.000000   \n",
            "2  https://www.indeed.com/rc/clk?jk=dcaae36c34971...      0.447214   \n",
            "3  https://www.indeed.com/rc/clk?jk=874d8ef92e7ed...      0.000000   \n",
            "4  https://www.indeed.com/rc/clk?jk=874d8ef92e7ed...      0.000000   \n",
            "\n",
            "         recommendation_3                                             link_3  \\\n",
            "0  Senior Account Manager  https://www.indeed.com/rc/clk?jk=271f286037b3b...   \n",
            "1            Data Analyst  https://www.indeed.com/rc/clk?jk=3fd69f471b790...   \n",
            "2        Delivery Analyst  https://www.indeed.com/rc/clk?jk=2a685b68a8e2b...   \n",
            "3            Data Analyst  https://www.indeed.com/rc/clk?jk=3fd69f471b790...   \n",
            "4            Data Analyst  https://www.indeed.com/rc/clk?jk=3fd69f471b790...   \n",
            "\n",
            "   similarity_3  \n",
            "0      0.447214  \n",
            "1      0.000000  \n",
            "2      0.447214  \n",
            "3      0.000000  \n",
            "4      0.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save df_jobs to a JSON file\n",
        "df_jobs.to_json(\"jobs_data.json\", orient=\"records\", lines=True)\n",
        "\n",
        "print(\"DataFrame has been saved as jobs_data.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DmtWXEWGIew",
        "outputId": "bc370ac1-8af7-4062-88db-3c0d4fd14344"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame has been saved as jobs_data.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model to an .h5 file\n",
        "model.save(\"model_binary_classification.h5\")\n",
        "print(\"Model has been saved as model_binary_classification.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IpjgmtnHiEt",
        "outputId": "ac5e5aea-3f31-4af6-ef2d-027a2b3e70a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model has been saved as model_binary_classification.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BUILD REQUIREMENT.TXT**"
      ],
      "metadata": {
        "id": "K04RPnkUUiPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the requirements.txt file manually\n",
        "with open('requirements.txt', 'w') as f:\n",
        "    f.write(\"pandas\\n\")\n",
        "    f.write(\"PyPDF2\\n\")\n",
        "    f.write(\"google-colab\\n\")\n",
        "    f.write(\"scikit-learn\\n\")\n",
        "    f.write(\"tensorflow\\n\")\n",
        "    f.write(\"transformers\\n\")\n",
        "\n",
        "# Download the requirements.txt file to your computer\n",
        "from google.colab import files\n",
        "files.download('requirements.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "emOtHBkvUlIv",
        "outputId": "d93132c3-5972-478b-8e38-fa2c101985f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_174b92e4-32b0-4fe0-b69f-5d7331a6d66a\", \"requirements.txt\", 64)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}